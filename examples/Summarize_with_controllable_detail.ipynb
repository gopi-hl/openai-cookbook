{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# open file\n",
    "with open(\"data/united_states_wikipedia.txt\", \"r\") as file:\n",
    "    united_states_wikipedia_text = file.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "16446"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "len(encoding.encode(united_states_wikipedia_text))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_chat_completion(messages, model='gpt-3.5-turbo'):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message['content']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 4361\n"
     ]
    }
   ],
   "source": [
    "def summarize(text: str,\n",
    "              detail: int,\n",
    "              model: str = 'gpt-3.5-turbo',\n",
    "              special_instructions: Optional[str] = None,\n",
    "              minimum_chunk_size: Optional[int] = 500,\n",
    "              chunk_delimiter: str = \".\",\n",
    "              summarize_recursively = False):\n",
    "    assert 0 <= detail <= 1\n",
    "    document_length = len(tokenize(text))\n",
    "    # interpolate chunk size between minimum_chunk_size and document_length // 2\n",
    "    chunk_size = int(minimum_chunk_size + detail * (document_length // 2 - minimum_chunk_size))\n",
    "    print(f\"Using a chunk size of {chunk_size} tokens\")\n",
    "    text_chunks = chunk_on_delimiter(text, chunk_size, chunk_delimiter)\n",
    "\n",
    "    accumulated_summaries = []\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        if summarize_recursively and accumulated_summaries:\n",
    "            # Creating a structured prompt for recursive summarization\n",
    "            system_message_content = \"Summarize the following text.\"\n",
    "            user_message_content = f\"Previous summaries:\\n\\n{'\\n\\n'.join(accumulated_summaries)}\\n\\nText to summarize next:\\n\\n{chunk}\"\n",
    "        else:\n",
    "            # Directly passing the chunk for summarization without recursive context\n",
    "            system_message_content = \"Summarize the following text.\"\n",
    "            user_message_content = chunk\n",
    "\n",
    "        # Constructing messages based on whether recursive summarization is applied\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message_content},\n",
    "            {\"role\": \"user\", \"content\": user_message_content}\n",
    "        ]\n",
    "\n",
    "        # Assuming this function gets the completion and works as expected\n",
    "        response = get_chat_completion(messages, model=model)\n",
    "        accumulated_summaries.append(response)\n",
    "\n",
    "    # Compile final summary from partial summaries\n",
    "    final_summary = '\\n\\n'.join(accumulated_summaries)\n",
    "\n",
    "    return final_summary, accumulated_summaries\n",
    "\n",
    "summarize(united_states_wikipedia_text, .5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "    return encoding.encode(text)\n",
    "\n",
    "\n",
    "def chunk_on_delimiter(input_string, max_tokens, delimiter):\n",
    "    chunks = input_string.split(delimiter)\n",
    "    combined_chunks, _, dropped_chunk_count = combine_chunks_with_no_minimum(\n",
    "        chunks, max_tokens, chunk_delimiter=delimiter, add_ellipsis=True\n",
    "    )\n",
    "    print(f\"warning: {dropped_chunk_count} chunks were dropped due to overflow\")\n",
    "    combined_chunks = [f\"{chunk}{delimiter}\" for chunk in combined_chunks]\n",
    "    return combined_chunks\n",
    "\n",
    "\n",
    "def combine_chunks_with_no_minimum(\n",
    "    chunks: List[str],\n",
    "    max_tokens: int,\n",
    "    chunk_delimiter=\"\\n\\n\",\n",
    "    header: Optional[str] = None,\n",
    "    add_ellipsis_for_overflow=False,\n",
    ") -> Tuple[List[str], List[int]]:\n",
    "    dropped_chunk_count = 0\n",
    "    output = []  # list to hold the final combined chunks\n",
    "    output_indices = []  # list to hold the indices of the final combined chunks\n",
    "    candidate = (\n",
    "        [] if header is None else [header]\n",
    "    )  # list to hold the current combined chunk candidate\n",
    "    candidate_indices = []\n",
    "    for chunk_i, chunk in enumerate(chunks):\n",
    "        chunk_with_header = [chunk] if header is None else [header, chunk]\n",
    "        if len(tokenize(chunk_delimiter.join(chunk_with_header))) > max_tokens:\n",
    "            print(f\"warning: chunk overflow\")\n",
    "            if (\n",
    "                add_ellipsis_for_overflow\n",
    "                and len(tokenize(chunk_delimiter.join(candidate + [\"...\"]))) <= max_tokens\n",
    "            ):\n",
    "                candidate.append(\"...\")\n",
    "                dropped_chunk_count += 1\n",
    "            continue  # this case would break downstream assumptions\n",
    "        # estimate token count with the current chunk added\n",
    "        extended_candidate_token_count = len(tokenize(chunk_delimiter.join(candidate + [chunk])))\n",
    "        # If the token count exceeds max_tokens, add the current candidate to output and start a new candidate\n",
    "        if extended_candidate_token_count > max_tokens:\n",
    "            output.append(chunk_delimiter.join(candidate))\n",
    "            output_indices.append(candidate_indices)\n",
    "            candidate = chunk_with_header  # re-initialize candidate\n",
    "            candidate_indices = [chunk_i]\n",
    "        # otherwise keep extending the candidate\n",
    "        else:\n",
    "            candidate.append(chunk)\n",
    "            candidate_indices.append(chunk_i)\n",
    "    # add the remaining candidate to output if it's not empty\n",
    "    if (header is not None and len(candidate) > 1) or (header is None and len(candidate) > 0):\n",
    "        output.append(chunk_delimiter.join(candidate))\n",
    "        output_indices.append(candidate_indices)\n",
    "    return output, output_indices, dropped_chunk_count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "[300,\n 301,\n 287,\n 225,\n 297,\n 288,\n 290,\n 295,\n 232,\n 299,\n 297,\n 289,\n 291,\n 273,\n 214,\n 297,\n 298,\n 288,\n 271,\n 266,\n 294,\n 291,\n 300,\n 295,\n 281,\n 301,\n 272,\n 289,\n 290,\n 292,\n 273,\n 289,\n 290,\n 273,\n 296,\n 262,\n 296,\n 301,\n 261,\n 291,\n 301,\n 300,\n 278,\n 300,\n 288,\n 249,\n 293,\n 268,\n 300,\n 296,\n 289,\n 299,\n 273,\n 263,\n 299,\n 293,\n 297,\n 267]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(tokenize(x)) for x in chunk_on_delimiter(united_states_wikipedia_text, 300, \".\")]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
